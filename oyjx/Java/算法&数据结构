数据结构：
复习前面的
    表、栈、队列、二叉树、二叉查找树、AVL树、BTree
    伸展树（可以以后详细了解）
红黑树（先不看，后面有时间再看）
    重要问题是提供对迭代器类的支持（困难部分是下一个节点高效地推进）
    《数据结构与算法分析：Java语言描述》P106
哈夫曼树（概念以及实现，回顾一下 = =）
    利用堆实现

散列
    散列表可以用来以常数平均时间实现insert和查找操作。
    当使用散列表时注意诸如装填因子这样的细节是特别重要的，否则时间界将不再有效；
    当关键字不是短的串或整数时，仔细选择散列函数也是很重要的！

    每个关键字被映射到从0到TableSize-1这个范围中的某个数，并且被放到适当的单元中。这个映射就叫作散列函数。

    分离链接法（散列到同一个值的所有元素保留在一个表中）

    不用链表的散列表：
        探测散列表（不能有冲突，所以不需要列表）
            注意点：保存的元素删除只能是懒惰删除，别的元素拿到同一个散列值但不能插入，可以进行递增直到该值没有存放元素位置
            这样能够保证同一元素多次散列是同一位置！

        双散列：
            @Todo

    再散列：
        1、表满到一半就再散列；
        2、一种极端的方式是当插入失败时才再散列；
        3、途中策略：当散列到达某一个装填因子时进行再散列。

     最坏情况O(1)访问的散列表：
        完美散列：
            @Todo
        布谷鸟散列（多散列表、更换散列方法（比如修改随机值）（rehash）、也还能够再散列）
        跳房子散列
            对经典的线性推测算法的改进。
            思路是，用事先确定的、对计算机的底层体系结构而言是最优的一个常数，给予探测序列的最大长度加上上界。这样做可以给出常数级的最坏查询时间，并且与布谷鸟散列一样，查询可以并行化，以同时检查可用位置的有限集。
            每一项额外记录下这项的实际散列值现在所处位置！

        布谷鸟散列和跳房子散列相对于经典的分离链接法和线性/二次探测法而言，是否能成为一种实际的替代算法，还有待观察。

     通用散列法：
        @Todo
        散列表表现和分析取决于散列函数具有以下两种性质：
            1、散列函数必须可在常数时间（即与表中项的个数无关）内计算；
            2、散列函数必须将各项均匀分布在数组单元中。

        散列函数不好，一切皆是徒劳，每个操作的花销可能都是线性的！

        一次对梅森素数（p=2^n - 1）的取模运算也可以用位移运算和一次加法来完成！
        卡特-韦格曼绝招的思路：
            5 % 3 = (5 / (3 + 1) * (3 + 1) + 5 % (3 + 1)) % 3)
                  = (1 * (3 + 1)) % 3 + 1 % 3
                  = 1 + 1 = 2
            注意：
                因为 x = (n + 1) * (x / (n + 1)) + (x % (n + 1))
                      = q * (n + 1) + r
                      q表示商，r表示余数
                q * (n + 1) % n = q，因为(n + 1) % n = 1。
                x % (n + 1) <= n，所以 (x % (n + 1)) % n = (x % (n + 1))，即x / (n + 1)的余数
                另外：(n + 1)如果是2^y，则计算q = x / (n + 1)可以通过x>>y得到结果；
                    余数可以通过按位与计算：x % 2^y = x & (2^y - 1)
                        原因：2^y的二进制是10000....的格式，2^-1的二进制是01111....的格式，按位与，1 & 1 才等于1。
                            不太好理解哈，除法得到商是看被除数可以减几次除数。 @Todo

     可扩散列：
        @Todo

     关于散列和二叉查找树的选择：
        如果不需要有序的信息以及对输入是否被排序存有怀疑，那么就应该选择散列这种数据结构。

     应用：
        编译器使用散列表跟踪源代码中声明的变量，这种数据结构叫作符号表（symbol table）。
        用于在游戏程序中，当程序搜索游戏不同的行时，它跟踪通过计算基于位置的散列函数而看到的一些位置（并把对于位置的移动存储起来），如果同样的位置再次出现，程序通常通过移动的简单变换来避免昂贵的重复计算，这种特征叫作转移表（transposition table）。
        散列表经常用于实现缓存，既在软件中，也在硬件中，它们还被用于路由器的硬件实现。

堆
    二叉堆（优先队列的一种实现）：
        结构性
            堆是一颗被完全填满的二叉树，有可能的例外是在底层，底层上的元素从左到右填入。
        堆序性（让操作快速执行的性质）
            在一个堆中，对于每一个节点X，X的父亲中的关键字小于（或等于）X中的关键字，根节点除外（他没有父亲）

        插入：
            一般的策略叫作上滤，新元素在堆中上滤直到找出正确的位置。
        删除最小元deleteMin()：
            一般的策略叫作下滤
        其他操作：
            降低关键字的值decreaseKey(p, x)：上滤
            增加关键字的值increaseKey(p, x)：下滤
            删除：可以首先执行decreaseKey(p, ∞)，然后再执行deleteMin()来完成
            构建堆buildHeap()：有时二叉堆是由一些项的初识集合构造而得。
                @Todo 参考堆排序
        删除和插入的代码疑问：
            @Todo 参考堆排序

    二叉堆的应用：
        选择问题：
            输入N个元素以及一个整数k，这N个元素的集可以是全序集。该选择问题是要找出第k个最大的元素：
            算法1A：
                排序所有元素，倒序，看第k个元素是哪个
            算法1B：
                取出k个元素排好序，最小者在k位置，然后剩余的元素与k比较，如果大于k则删除k，然后排放在原k之前的合适位置，算法结束之时，第k个位置上的元素就是问题的解了。（类似插入排序，先拿k张牌，排好序，然后抽余下的牌，比手里最小的要大的则替换掉，并排好序，不同的是手里的排一直只有k个！）
            算法6A：
                改变二叉堆堆序性质，即最大元素的在根，然后删除最大值k次，即直到第k个最大的元素了。
            算法6B：
                通过堆实现1B，前k个元素调用一个buildHeap以总时间O(k)被置入堆中，处理每个其余的元素的时间是O(1)，用于检测是否比k个元素中最小的要小，再加上时间O(log(k)，用于在必要时删除k个元素中最小的以及插入新元素。因此总时间是O(k + (N - k)log(k))=O(log(k))。
        事件模拟：
            @Todo

     d-堆：
        就像是一个二叉堆，只是所有的节点都有d个儿子（因此，二叉堆是2-堆）
        @Todo
     左式堆：
        @Todo
     斜堆：
        @Todo
     二项队列：
        @Todo

算法：

排序算法 =>

关于稳定性：
    即相同值的元素排序前后相对位置不会改变！
关于辅助空间：
    那是执行算法时必须要的空间,如果大小的数量级为1,换句话说该算法所需的辅助空间不依赖算法的规模。

冒泡排序：
    原理：就像冒泡一样，第 一轮冒出最大的，第二轮冒出第二大的，以此类推，所以里面的循环次数越来越少！
    稳定、平均/最好/最坏情况O(n^2)、辅助空间O(1)
    代码：
        for (int i = 0; i < arrays.length; i++) {
            // 每一趟都能排出一个最大的，所以里层循环的次数可以 - i
            for (int j = 0; j < arrays.length - i - 1; j++) {
                if (arrays[j] > arrays[j + 1]) { // 如果条件改成arrays[j] >= arrays[j + 1]，则变为不稳定的排序算法
                    int temp = arrays[j + 1];
                    arrays[j + 1] = arrays[j];
                    arrays[j] = temp;
                }
            }
        }

选择排序：（看起来与冒泡很像）
    原理：初始时在序列中找到最小（大）元素，放到序列的起始位置作为已排序序列；然后，再从剩余未排序元素中继续寻找最小（大）元素，放到已排序序列的末尾。以此类推，直到所有元素均排序完毕。
    不稳定、平均/最好/最坏情况O(n^2)、辅助空间O(1)
        选择排序是不稳定的排序算法，不稳定发生在最小元素与A[i]交换的时刻。
        如序列：{ 5, 8, 5, 2, 9 }，一次选择的最小元素是2，然后把2和第一个5进行交换，从而改变了两个元素5的相对次序。
    代码：
        /**
         *  从第一个元素开始，每一次循环找出最小的元素，第二次循环找出第二小的元素，以此类推
         *  注意选择排序与冒泡排序的区别：
         *      冒泡排序通过依次交换相邻两个顺序不合法的元素位置，从而将当前最小（大）元素放到合适的位置；
         *      而选择排序每遍历一次都记住了当前最小（大）元素的位置，最后仅需一次交换操作即可将其放到合适的位置
         */
        for (int i = 0; i < arrays.length - 1; i++) {
            int min = i;
            // 找出比arrays[i]还小的元素，从i的下一个开始找，因为前面的已经排好了序
            for (int j = i + 1; j < arrays.length; j++) {
                if (arrays[j] < arrays[min]) {
                    min = j;
                }
            }
            // 如果有比arrays[i]下的元素，则交换
            if (min != i) {
                int temp = arrays[i];
                arrays[i] = arrays[min];
                arrays[min] = temp;
            }
        }

插入排序：
    原理：和摸扑克牌一样，手里的永远是排好序的，摸一张新的和手里的比较好，确定位置，直到摸完最后一样，手里的都排好序了！
    特点：稳定（等于不置换）、平均和最坏情况O(n^2)、最好情况O(n)、、辅助空间O(1)
        最好情况：对已经排好序的，比如每次摸的都比手上最大的牌大，那岂不是爽歪歪
    伪代码：
        for j = 1 to A.length - 1
            i = j - 1
            // 先和大的比较，往前比，比key大的与之交换
            while i >= 0 and A[i] > key
                A[i + 1] = A[i]
                i = i - 1
            A[i + 1] = key; // 确定位置之后才放置key

希尔排序（缩减增量排序）：
    原理：通过比较相距一定间隔的元素来工作；各趟比较所用的距离随着算法的进行而减小，直到只比较相邻元素的最后一趟排序为止。
        可以看成是一个更高效的插入排序改进版本（多debug走走就好理解些，另外要明白增量对最终排序正确是没影响的，以及它的好处在哪？）
    不稳定、平均O(nlog(n))~O(n^2)、最好O(n^1.3)、最坏情况O(n^2)
        比如序列：{ 3, 5, 10, 8, 7, 2, 8, 1, 20, 6 }，h=2时分成两个子序列 { 3, 10, 7, 8, 20 } 和  { 5, 8, 2, 1, 6 } ，未排序之前第二个子序列中的8在前面，现在对两个子序列进行插入排序，得到 { 3, 7, 8, 10, 20 } 和 { 1, 2, 5, 6, 8 } ，即 { 3, 1, 7, 2, 8, 5, 10, 6, 20, 8 } ，两个8的相对次序发生了改变。
    代码：
        // 生成初始增量
        int h = 0;
        while (h <= arrays.length) {
            h = 3 * h + 1;
        }
        System.out.println();
        while (h >= 1){
            System.out.println(h);
            for (int i = h; i < arrays.length; i++) {
                // 里面这层其实是插入排序的实现（不好理解的话把h想象成1）
                // 这里把里层和外层循环分成两拨，i不一定从1开始，比如从4开始，去与前面排好序的数组比较，如果小则交换下
                // 只是如果找到一个更小的交换后，不一定比较前一个，而是前h个，可能会错过，但没关系，h会递减的，下次再来（h的范围变下，外层的就变大，也就是错过的就能够重新比较）
                int key = arrays[i];
                int j = i - h; // 从0开始
                while (j >= 0 && arrays[j] > key) {
                    arrays[j + h] = arrays[j];
                    j = j - h;
                }
                arrays[j + h] = key;
            }
            // 递减增量
            h = (h - 1) / 3;
        }

堆排序：
    原理：可以避免使用附加的数组就是在删除堆元素后把该元素放到数组末尾，末尾-1，以此类推，这样如果是最用小堆则是逆序的，用最大堆则是顺序的
        通过deleteMin或deleteMax得到排序，二叉堆每个deleteMin的时间是O(log(n))，n个元素操作完之后总时间是O(nlog(n))。
                数组实现二叉堆的规律
                         0
                      1     2
                     3 4   5 6
             n 为某父节点，其左节点：(n*i+1) ；其右节点：(n*i+1+1)
    不稳定、平均/最好/最坏情况O(nlog(n))、辅助空间O(1)
        如序列：{ 9, 5, 7, 5 }，堆顶元素是9，堆排序下一步将9和第二个5进行交换，得到序列 { 5, 5, 7, 9 }，再进行堆调整得到{ 7, 5, 5, 9 }，重复之前的操作最后得到{ 5, 5, 7, 9 }从而改变了两个5的相对次序。
    代码：
        参考oyjxDemo下的com.original.java.algorithm.sort.HeapSortDemo.java

归并排序（分治算法）：
    原理：借助临时数组将两份排好序的数据放入该临时数组，我们将数组分成最小份（1个或2个），每一份排好序，再往上合，再次排序，以此类推。
                        比如：[2, 7, 4, 6, 3, 8]
                                2 7 4 6 3 8
                             2 7 4       6 3 8
                            2   7 4     6   3 8
                 最小份：   2    7   4   6   3   8
    从最小份开始排序与合并:  2       4 7  6     3 8
                            2 4 7        3 6 8
                                2 3 4 6 7 8
                                  排序完成

    每次组合都是两个排好序的部分通过一个临时数组组成一个排好序一部分，然后临时数组的这一整部分替换掉源数组的两部分。

    稳定、平均/最好/最坏情况O(nlog(n))、辅助空间O(n)
    代码：
        参考oyjxDemo下的com.original.java.algorithm.sort.MergeSortDemo.java

快速排序：
    原理：
        通过集合实现的分治递归算法
            取中值，分三组，smaller、same、larger，smaller、larger再递归再分，直到最小只有两个元素，将之按smaller、same、larger放到集合。
        实际的快速排序实现：
            主要思想：取枢纽元，划分左右（保证左边的小于枢纽元或等于），右边的大于枢纽元，然后左右再次递归直至剩下一个元素

        注意：对于小数组（N <= 20），快速排序不如插入排序。不仅如此，因为快速排序是递归的，所以这样的情形经常发生。通常的解决方法是对于小的数组不使用快速排序，而代之以诸如插入排序这样的对小数组有效的排序算法。

    不稳定、平均/最好O(nlog(n))、最坏情况O(n^2)、辅助空间O(log(n))~O(n)
        比如序列：{ 1, 3, 4, 2, 8, 9, 8, 7, 5 }，基准元素是5，一次划分操作后5要和第一个8进行交换，从而改变了两个元素8的相对次序。
    代码：
        参考oyjxDemo下的com.original.java.algorithm.sort.QuickSortDemo.java

@Todo
桶排序：
基数排序：

== 算法
图论算法
贪婪算法
    哈夫曼树（哈夫曼算法）

分治算法
    归并排序、快速排序的实现

    分治算法由两部分组成：
        分：递归解决较小的问题（当然，基本情况除外）
        治：然后从子问题的解构建原问题的解

动态规划

随机化算法
    比如一个学期n次的作业，每次每个学生类似抛硬币，有50%的可能性需要检查，一个学期期望的情况是每个学生有一半的作业是要检查的
    在算法期间，随机数至少有一次用于决策，该算法的运行时间不只依赖于特定的输入，还依赖于所出现的随机数。

回朔算法
    很多情况下，回溯算法相当于是穷举搜索的巧妙实现，但性能一般不理想


算法复杂度@Todo

算法书上关于时间复杂度的分析，最好最坏情况的分析可以仔细看看！！！@Todo

常数时间的操作：一个操作如果和数据量没有关系，每次都是固定时间内完成的操作，叫做常数操作。

时间复杂度为一个算法流程中，常数操作数量的指标。常用O（读作big O）来表示。具体来说，在常数操作数量的表达式中，只要高阶项，不要低阶项，也不要高阶项的系数，剩下的部分，如果记为f(N)，那么时间复杂度为O(f(N))。

一般我们是先拿到f(N)，然后来算下他的时间复杂度，一般我们只保留对函数增长速度较大的函数
例如:
1、f(N)=c（c是常数），我们称时间复杂度为O(1)
2、f(N)=a*N+b(a和b是常数)，我们称时间复杂度为O(N)
3、f(N)=a*N^2+b*N+c(a,b,c均为常数)，我们称时间复杂度为O(N^2)
4、f(N)=a*N^2*logN+b*N+c(a,b,c均为常数)，我们称时间复杂度为O(N^2*logN)


例如对二分查找的分析：
由于二分查找每次查询都是从数组中间切开查询，所以每次查询，剩余的查询数为上一次的一半，从下表可以清晰的看出查询次数与剩余元素数量对应关系

表-查询次数及剩余数：
第几次查询	剩余待查询元素数量
1	            N/2
2	            N/(2^2)
3	            N/(2^3)
…	            …
K	            N/(2^K)
从上表可以看出N/(2^K)肯定是大于等于1，也就是N/(2^K)>=1，我们计算时间复杂度是按照最坏的情况进行计算，也就是是查到剩余最后一个数才查到我们想要的数据，也就是
N/(2^K)=1
=>N=2^K
=>K=log2N
所以二分查找的时间复杂度为O(log2N)。

关于log2 N：
比值为log2 N / log3 N，运用换底公式后得：(lnN/ln2) / (lnN/ln3) = ln3 / ln2，ln为自然对数，显然这三个常数，与变量N无关。
用文字表述：算法时间复杂度为log（n）时，不同底数对应的时间复杂度的倍数关系为常数，不会随着底数的不同而不同，因此可以将不同底数的对数函数所代表的时间复杂度，当作是同一类复杂度处理，即抽象成一类问题。
